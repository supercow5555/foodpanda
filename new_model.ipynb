{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(906, 4)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn import over_sampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# from transformers import TFDistilBertFoerSequenceClassification\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_items = pd.read_csv(r'C:\\Users\\kkaus\\OneDrive - HKUST Connect\\Foodpanda\\Data\\item_list.csv')\n",
    "df_outlets = pd.read_csv(r'C:\\Users\\kkaus\\OneDrive - HKUST Connect\\Foodpanda\\Data\\outlet_visits.csv')\n",
    "food_database = pd.read_csv(r\"C:\\Users\\kkaus\\OneDrive\\Documents\\GitHub\\foodpanda-project\\data\\generic-food.csv\")\n",
    "\n",
    "df_items.shape\n",
    "food_database.shape\n",
    "\n",
    "\n",
    "array(['Middle Eastern', 'Malaysian', 'Singaporean', 'German', 'French',\n",
    "       'Indonesian', 'Mexican', 'Spanish', 'Mediterranean', 'European',\n",
    "       'British', 'Thai', 'Indian', 'Korean', 'Chinese', 'Italian',\n",
    "       'American', 'Japanese', 'Cantonese', 'Hong Kong', 'Taiwanese',\n",
    "       'Vietnamese', 'International'], dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Middle Eastern', 'Malaysian', 'Singaporean', 'German', 'French',\n",
       "       'Indonesian', 'Mexican', 'Spanish', 'Mediterranean', 'European',\n",
       "       'British', 'Thai', 'Indian', 'Korean', 'Chinese', 'Italian',\n",
       "       'American', 'Japanese', 'Cantonese', 'Hong Kong', 'Taiwanese',\n",
       "       'Vietnamese', 'International'], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonan = df_items[df_items['primary_cuisine'].notna()]\n",
    "df_clean = df_nonan[df_nonan['primary_cuisine'] .str.contains(\"Fast Food|Dessert|Healthy|Drinks|Pizza|Burgers|Sandwich|Halal|Noodles|Cakes & Bakery|Sichuan|Western|Chiu Chow|Shanghainese|Asian|Beijing|South American\") ==  False]\n",
    "df_clean.primary_cuisine.unique()\n",
    "# df_sample = df_clean.sample(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkaus\\AppData\\Local\\Temp/ipykernel_15344/1082737213.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[\"x\"] = df_clean[\"brand_name\"].fillna('') + \" \" + df_clean[\"product_title\"].fillna('') + \" \" +  df_clean[\"description\"].fillna('')\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"x\"] = df_clean[\"brand_name\"].fillna('') + \" \" + df_clean[\"product_title\"].fillna('') + \" \" +  df_clean[\"description\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (506124,)\n",
      "y_train: (506124,)\n",
      "X_valid: (63266,)\n",
      "y_valid: (63266,)\n",
      "X_test: (63266,)\n",
      "y_test: (63266,)\n"
     ]
    }
   ],
   "source": [
    "X = df_clean[\"x\"]#.sample(100000, random_state=1)\n",
    "y = df_clean[\"primary_cuisine\"]#.sample(100000, random_state=1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# X = X.to_numpy()\n",
    "\n",
    "# Splitting\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size = 0.80)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size= 0.5)\n",
    "# checking proportions\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"y_valid:\", y_valid.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(X_train.values)\n",
    "np.unique(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['American', 'British', 'Cantonese', 'Chinese', 'European',\n",
       "       'French', 'German', 'Hong Kong', 'Indian', 'Indonesian',\n",
       "       'International', 'Italian', 'Japanese', 'Korean', 'Malaysian',\n",
       "       'Mediterranean', 'Mexican', 'Middle Eastern', 'Singaporean',\n",
       "       'Spanish', 'Taiwanese', 'Thai', 'Vietnamese'], dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Café de Coral BBQ Pork with Rice (Small)‧Chicken Wing Served with tea/coffee/soft drink'],\n",
       "       ['Praise Dining Bistro 545. Japanese Style Sauteed Diced Beef '],\n",
       "       [\"Daniel's Restaurant S32. BBQ Pork with Instant Noodles \"],\n",
       "       ...,\n",
       "       ['Shing Shun Seafood Restaurant 8201. Steak Cutlet with Brown Sauce '],\n",
       "       ['Supreme Sashimi Combachi Sushi (1 pc) '],\n",
       "       ['Po Lam Cafe Grilled Wok-fried Beef Tenderloin Set ']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target statistics: Counter({7: 230117, 12: 55672, 0: 53446, 20: 37023, 3: 37007, 21: 19498, 10: 17396, 2: 10999, 22: 9585, 8: 9381, 13: 9337, 11: 4915, 18: 2196, 14: 2139, 17: 1991, 16: 1404, 4: 1154, 5: 1027, 9: 415, 19: 412, 15: 396, 6: 314, 1: 300})\n",
      "Testing target statistics: Counter({7: 28779, 12: 7055, 0: 6551, 3: 4686, 20: 4615, 21: 2402, 10: 2191, 2: 1319, 8: 1254, 22: 1216, 13: 1180, 11: 568, 18: 285, 14: 260, 17: 229, 16: 189, 4: 146, 5: 110, 19: 60, 15: 52, 1: 48, 9: 37, 6: 34})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(f\"Training target statistics: {Counter(y_train)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# over_sampler = RandomOverSampler(random_state=1)\n",
    "\n",
    "\n",
    "# X_res, y_res = over_sampler.fit_resample(X_train.reshape(-1,1), y_train)\n",
    "# print(f\"Training target statistics: {Counter(y_res)}\")\n",
    "# # print(f\"Testing target statistics: {Counter(y_test)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target statistics: Counter({0: 300, 1: 300, 2: 300, 3: 300, 4: 300, 5: 300, 6: 300, 7: 300, 8: 300, 9: 300, 10: 300, 11: 300, 12: 300, 13: 300, 14: 300, 15: 300, 16: 300, 17: 300, 18: 300, 19: 300, 20: 300, 21: 300, 22: 300})\n",
      "Testing target statistics: Counter({7: 28779, 12: 7055, 0: 6551, 3: 4686, 20: 4615, 21: 2402, 10: 2191, 2: 1319, 8: 1254, 22: 1216, 13: 1180, 11: 568, 18: 285, 14: 260, 17: 229, 16: 189, 4: 146, 5: 110, 19: 60, 15: 52, 1: 48, 9: 37, 6: 34})\n"
     ]
    }
   ],
   "source": [
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_res1, y_res1 = under_sampler.fit_resample(X_train.reshape(-1,1), y_train)\n",
    "print(f\"Training target statistics: {Counter(y_res1)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15344/3967851687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Getting max string length in X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMAX_LEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# Getting max string length in X\n",
    "\n",
    "MAX_LEN = X_train.apply(lambda s: len([x for x in s.split()])).max()\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode with  DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a tokenizer object\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_res1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_res1.shape\n",
    "# list(X_valid.values)\n",
    "X_train = pd.Series(X_res1)\n",
    "X_valid = pd.Series(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15344/3366326116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tokenize the text (padding to max sequence in batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_res1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvalidation_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\comp4331\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2453\u001b[0m                 )\n\u001b[0;32m   2454\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2455\u001b[1;33m             return self.batch_encode_plus(\n\u001b[0m\u001b[0;32m   2456\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2457\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\comp4331\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2644\u001b[0m         )\n\u001b[0;32m   2645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\comp4331\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#tokenize the text (padding to max sequence in batch)\n",
    "train_encodings = tokenizer((X_res1.tolist()), truncation=True, padding=True)\n",
    "validation_encodings = tokenizer(list(X_valid.values), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(X_test.values), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encodings_array = np.array(train_encodings[\"input_ids\"])\n",
    "X_valid_encodings_array = np.array(validation_encodings[\"input_ids\"])\n",
    "X_test_encodings_array = np.array(test_encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encodings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first paragraph and it transformation\n",
    "print(f'First paragraph: \\'{X_train[:1]}\\'')\n",
    "print(f'Input ids: {train_encodings[\"input_ids\"][0]}')\n",
    "print(f'Attention mask: {train_encodings[\"attention_mask\"][0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_encodings[\"input_ids\"]).hist();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings[\"attention_mask\"][0]) #max len tokenized sentence - 258\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Turn our labels and encodings into a tf.Dataset object\n",
    "\n",
    "For some reason this doesn't work, it sounds really cool but can't actually figure out how to utilize this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n",
    "#                                                     list(y_train)))\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n",
    "#                                                     list(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting into a feedforward neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_feed_forward\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                16576     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                1560      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,296\n",
      "Trainable params: 22,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(name=\"simple_feed_forward\")\n",
    "model.add(tf.keras.Input(shape=(258,)))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(24, activation=tf.nn.softmax))\n",
    "# model.output_shape\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)                     # Computes the crossentropy loss between the labels and predictions. \n",
    "model.compile(optimizer=optimizerr,                                     \n",
    "              loss=losss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_encodings_array, y_train, \n",
    "          epochs=N_EPOCHS,\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "max_seq_length 128 \\\n",
    "per_device_train_batch_size 16 \\\n",
    "learning_rate 2e-5 \\\n",
    "num_train_epochs 5 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model\n",
    "model.evaluate(X_valid_encodings_array, y_valid, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_valid_encodings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = prediction[243]\n",
    "(np.where(guess==1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[243]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(prediction,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):\n",
    "  \"\"\"\n",
    "  To get array with predicted probabilities for 0 - 23 : All primary cuisines\n",
    "  :param text_list: list[str]\n",
    "  :param model: transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification\n",
    "  :param tokenizer: transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer\n",
    "  :return res: numpy.ndarray\n",
    "  \"\"\"\n",
    "     \n",
    "  encodings = tokenizer(text_list, truncation=True, padding=True)\n",
    "  encodings_array = np.array(encodings[\"input_ids\"])\n",
    "  preds = model.predict(encodings_array)                                          # Gives a 0-23 0s and 1s\n",
    "  # res = tf.nn.softmax(preds, axis=1).numpy()                                # No clue what it does\n",
    "  encodings_array\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba(list(X), model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f7248dda44c9aca490d825e08e1c8b901064e5575a7eb735a794e1a698a65ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('comp4331')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
